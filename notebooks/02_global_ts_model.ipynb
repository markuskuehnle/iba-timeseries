{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 22:29:53,460] A new study created in memory with name: no-name-d978315b-c0c9-4057-9b46-a0f8af854af5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa6e0cdf91043e89ea09c55cb81363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 22:29:55,177] Trial 0 finished with value: 11.176479343716721 and parameters: {'max_depth': 9, 'learning_rate': 0.27860476988145516, 'subsample': 0.5585388479136153, 'colsample_bytree': 0.756262202262777, 'n_estimators': 201}. Best is trial 0 with value: 11.176479343716721.\n",
      "[I 2025-04-23 22:29:57,032] Trial 1 finished with value: 10.664759540617956 and parameters: {'max_depth': 6, 'learning_rate': 0.07917615836556838, 'subsample': 0.6599234049749177, 'colsample_bytree': 0.5863936511111316, 'n_estimators': 325}. Best is trial 1 with value: 10.664759540617956.\n",
      "[I 2025-04-23 22:29:58,465] Trial 2 finished with value: 9.455923782853079 and parameters: {'max_depth': 5, 'learning_rate': 0.08025879108764446, 'subsample': 0.8602068824456328, 'colsample_bytree': 0.699945804709013, 'n_estimators': 295}. Best is trial 2 with value: 9.455923782853079.\n",
      "[I 2025-04-23 22:30:00,286] Trial 3 finished with value: 8.709272316788171 and parameters: {'max_depth': 10, 'learning_rate': 0.055073488206559586, 'subsample': 0.8471141018467565, 'colsample_bytree': 0.920978051466218, 'n_estimators': 207}. Best is trial 3 with value: 8.709272316788171.\n",
      "[I 2025-04-23 22:30:02,758] Trial 4 finished with value: 11.903090489495918 and parameters: {'max_depth': 7, 'learning_rate': 0.2004526827880494, 'subsample': 0.9574971374121035, 'colsample_bytree': 0.7853836385578832, 'n_estimators': 376}. Best is trial 3 with value: 8.709272316788171.\n",
      "[I 2025-04-23 22:30:04,232] Trial 5 finished with value: 7.9054654690673205 and parameters: {'max_depth': 8, 'learning_rate': 0.1367428844780831, 'subsample': 0.7605576642910286, 'colsample_bytree': 0.6427352910155788, 'n_estimators': 198}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:06,737] Trial 6 finished with value: 8.572671252264957 and parameters: {'max_depth': 7, 'learning_rate': 0.07220529123120309, 'subsample': 0.8881358807416084, 'colsample_bytree': 0.6963480570813074, 'n_estimators': 379}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:09,953] Trial 7 finished with value: 12.280616113082703 and parameters: {'max_depth': 10, 'learning_rate': 0.05268166907664926, 'subsample': 0.5153555970326649, 'colsample_bytree': 0.5108830069136596, 'n_estimators': 365}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:12,815] Trial 8 finished with value: 10.780423794645918 and parameters: {'max_depth': 9, 'learning_rate': 0.14725724725892184, 'subsample': 0.9995515908756187, 'colsample_bytree': 0.7018523831326748, 'n_estimators': 343}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:15,947] Trial 9 finished with value: 12.1733152279852 and parameters: {'max_depth': 9, 'learning_rate': 0.2542212331727678, 'subsample': 0.6425419601729849, 'colsample_bytree': 0.963305461398394, 'n_estimators': 377}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:16,824] Trial 10 finished with value: 45.60241915696181 and parameters: {'max_depth': 12, 'learning_rate': 0.012727790794139505, 'subsample': 0.7599244139498922, 'colsample_bytree': 0.8585853189342232, 'n_estimators': 120}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:18,280] Trial 11 finished with value: 9.254524330564147 and parameters: {'max_depth': 3, 'learning_rate': 0.029166915240157208, 'subsample': 0.8215670413545295, 'colsample_bytree': 0.6177232887636523, 'n_estimators': 450}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:20,769] Trial 12 finished with value: 9.787946975060951 and parameters: {'max_depth': 5, 'learning_rate': 0.117452460702606, 'subsample': 0.9079543932191859, 'colsample_bytree': 0.6342302979698156, 'n_estimators': 498}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:22,409] Trial 13 finished with value: 9.452798935887884 and parameters: {'max_depth': 7, 'learning_rate': 0.030092940305519726, 'subsample': 0.7541144236300447, 'colsample_bytree': 0.8231251629156144, 'n_estimators': 245}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:23,490] Trial 14 finished with value: 11.67724560920689 and parameters: {'max_depth': 8, 'learning_rate': 0.12491593981832601, 'subsample': 0.6806714416828809, 'colsample_bytree': 0.6736753631023209, 'n_estimators': 142}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:24,350] Trial 15 finished with value: 12.470210988120963 and parameters: {'max_depth': 3, 'learning_rate': 0.034288659482942074, 'subsample': 0.7980731612108586, 'colsample_bytree': 0.5362119315043544, 'n_estimators': 267}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:28,874] Trial 16 finished with value: 9.742847778526912 and parameters: {'max_depth': 12, 'learning_rate': 0.07878451072962253, 'subsample': 0.8879312287409535, 'colsample_bytree': 0.573870078270588, 'n_estimators': 420}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:29,739] Trial 17 finished with value: 22.871878181962433 and parameters: {'max_depth': 5, 'learning_rate': 0.015347379768862168, 'subsample': 0.6964122968452241, 'colsample_bytree': 0.6634222984699316, 'n_estimators': 174}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:31,208] Trial 18 finished with value: 9.473607656630229 and parameters: {'max_depth': 6, 'learning_rate': 0.15241454326057893, 'subsample': 0.9290372477777149, 'colsample_bytree': 0.7315758497092197, 'n_estimators': 253}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:34,432] Trial 19 finished with value: 10.854686000475136 and parameters: {'max_depth': 8, 'learning_rate': 0.04823749666016178, 'subsample': 0.5906714192056566, 'colsample_bytree': 0.8600082274248028, 'n_estimators': 437}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:37,397] Trial 20 finished with value: 10.782946122745434 and parameters: {'max_depth': 11, 'learning_rate': 0.10183194036606548, 'subsample': 0.7875455224766456, 'colsample_bytree': 0.7785135855626103, 'n_estimators': 298}. Best is trial 5 with value: 7.9054654690673205.\n",
      "[I 2025-04-23 22:30:39,241] Trial 21 finished with value: 7.287525536819085 and parameters: {'max_depth': 10, 'learning_rate': 0.04500373016119465, 'subsample': 0.8419360052485325, 'colsample_bytree': 0.9258709891959414, 'n_estimators': 207}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:40,557] Trial 22 finished with value: 11.50421223520332 and parameters: {'max_depth': 10, 'learning_rate': 0.020254980205061522, 'subsample': 0.7161903124539635, 'colsample_bytree': 0.9972265319904958, 'n_estimators': 169}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:41,327] Trial 23 finished with value: 8.54882256164134 and parameters: {'max_depth': 8, 'learning_rate': 0.04082437488861635, 'subsample': 0.8445175477196254, 'colsample_bytree': 0.8943657966019811, 'n_estimators': 102}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:42,190] Trial 24 finished with value: 7.758283509608337 and parameters: {'max_depth': 11, 'learning_rate': 0.04078390404188529, 'subsample': 0.8376665676217006, 'colsample_bytree': 0.9119403686573789, 'n_estimators': 102}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:44,029] Trial 25 finished with value: 7.484303119943355 and parameters: {'max_depth': 11, 'learning_rate': 0.022141005340209772, 'subsample': 0.738342541509796, 'colsample_bytree': 0.9371913680630786, 'n_estimators': 218}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:45,933] Trial 26 finished with value: 7.48834295876117 and parameters: {'max_depth': 11, 'learning_rate': 0.021755231414332558, 'subsample': 0.729151162080903, 'colsample_bytree': 0.9082956641202711, 'n_estimators': 227}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:47,762] Trial 27 finished with value: 10.668746017519332 and parameters: {'max_depth': 11, 'learning_rate': 0.020898367148761688, 'subsample': 0.6133315719387082, 'colsample_bytree': 0.9529912245552965, 'n_estimators': 224}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:48,988] Trial 28 finished with value: 38.93652794419176 and parameters: {'max_depth': 12, 'learning_rate': 0.010070478243558757, 'subsample': 0.7125575732705031, 'colsample_bytree': 0.8577274808767625, 'n_estimators': 173}. Best is trial 21 with value: 7.287525536819085.\n",
      "[I 2025-04-23 22:30:50,870] Trial 29 finished with value: 7.828566778498601 and parameters: {'max_depth': 11, 'learning_rate': 0.020035411361987682, 'subsample': 0.7940485375407561, 'colsample_bytree': 0.9852472758967218, 'n_estimators': 227}. Best is trial 21 with value: 7.287525536819085.\n",
      "Best hyperparameters: {'max_depth': 10, 'learning_rate': 0.04500373016119465, 'subsample': 0.8419360052485325, 'colsample_bytree': 0.9258709891959414, 'n_estimators': 207}\n",
      "Hold‐out Test (6 mo): MAE=138446276, MAPE=10.21%\n",
      "Saved tuned model to ../models/global_xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_pipeline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "# -----------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def time_series_splits(df, n_folds=3, val_size=6):\n",
    "    \"\"\"Yield (train_df, val_df) forward‐chaining splits.\"\"\"\n",
    "    n = len(df)\n",
    "    for i in range(n_folds):\n",
    "        train_end = n - (n_folds - i) * val_size\n",
    "        val_start = train_end\n",
    "        val_end   = val_start + val_size\n",
    "        if val_end > n:\n",
    "            break\n",
    "        yield df.iloc[:train_end], df.iloc[val_start:val_end]\n",
    "\n",
    "def load_and_preprocess(data_path):\n",
    "    \"\"\"\n",
    "    Load CSV, filter dates, aggregate to monthly, create\n",
    "    the same features & dropped columns as in training.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path, low_memory=False)\n",
    "    df[\"Kalendertag\"]    = pd.to_datetime(df[\"Kalendertag\"], errors=\"coerce\")\n",
    "    df[\"Eintrittsdatum\"] = pd.to_datetime(df[\"Eintrittsdatum\"], errors=\"coerce\")\n",
    "    df = df[df[\"Kalendertag\"] >= df[\"Eintrittsdatum\"]]\n",
    "    df[\"JahrMonat\"]      = pd.to_datetime(df[\"Kalendertag\"].dt.to_period(\"M\").astype(str))\n",
    "\n",
    "    monthly = (\n",
    "        df.groupby(\"JahrMonat\")\n",
    "          .agg({\n",
    "            \"BIWNAV AV Neug Wesu\":\"sum\",\n",
    "            \"Anz. NeuFamilien\":    \"sum\",\n",
    "            \"Anz. Neukunden\":      \"sum\",\n",
    "            \"P AV neu\":            \"sum\",\n",
    "            \"SpB AV\":              \"sum\",\n",
    "          })\n",
    "          .reset_index()\n",
    "          .rename(columns={\"BIWNAV AV Neug Wesu\":\"target\"})\n",
    "          .sort_values(\"JahrMonat\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Feature engineering\n",
    "    monthly[\"month\"]     = monthly[\"JahrMonat\"].dt.month\n",
    "    monthly[\"month_sin\"] = np.sin(2 * np.pi * monthly[\"month\"] / 12)\n",
    "    monthly[\"month_cos\"] = np.cos(2 * np.pi * monthly[\"month\"] / 12)\n",
    "    for col in [\"Anz. NeuFamilien\",\"Anz. Neukunden\",\"P AV neu\",\"SpB AV\"]:\n",
    "        monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "        monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "    monthly[\"target_t_plus_1\"] = monthly[\"target\"].shift(-1)\n",
    "\n",
    "    monthly = (\n",
    "        monthly\n",
    "        .drop(columns=[\n",
    "            \"month\",\n",
    "            \"Anz. NeuFamilien\",\"Anz. Neukunden\",\"P AV neu\",\"SpB AV\",\n",
    "            \"target\"\n",
    "        ])\n",
    "        .dropna(subset=[\"target_t_plus_1\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return monthly\n",
    "\n",
    "# -----------------------------------\n",
    "# Main orchestration\n",
    "# -----------------------------------\n",
    "def main(TEST_SIZE=6, N_TRIALS=30):\n",
    "    DATA_PATH  = \"../data/4_PrognoseCase_AV_NG_-_Daten.csv\"\n",
    "    MODEL_PATH = \"../models/global_xgb_model.pkl\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "    monthly       = load_and_preprocess(DATA_PATH)\n",
    "    train_val_df  = monthly.iloc[:-TEST_SIZE]\n",
    "    test_df       = monthly.iloc[-TEST_SIZE:]\n",
    "    feature_cols  = [c for c in monthly.columns if c not in [\"JahrMonat\",\"target_t_plus_1\"]]\n",
    "\n",
    "    # 1) Optuna hyperparameter tuning\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\":        \"reg:squarederror\",\n",
    "            \"tree_method\":      \"hist\",\n",
    "            \"random_state\":     42,\n",
    "            \"verbosity\":        0,\n",
    "            \"max_depth\":        trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\":        trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"n_estimators\":     trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        }\n",
    "        mape_scores = []\n",
    "        for tr_df, val_df in time_series_splits(train_val_df, n_folds=3, val_size=TEST_SIZE):\n",
    "            X_tr, y_tr   = tr_df[feature_cols], tr_df[\"target_t_plus_1\"]\n",
    "            X_val, y_val = val_df[feature_cols], val_df[\"target_t_plus_1\"]\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            preds = model.predict(X_val)\n",
    "            mape_scores.append(mean_absolute_percentage_error(y_val, preds))\n",
    "        return float(np.mean(mape_scores))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    tuned = {\n",
    "        **best_params,\n",
    "        \"objective\":    \"reg:squarederror\",\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\":    0,\n",
    "    }\n",
    "\n",
    "    # 2) Train on train_val and evaluate on hold-out\n",
    "    prod_model = XGBRegressor(**tuned)\n",
    "    prod_model.fit(train_val_df[feature_cols], train_val_df[\"target_t_plus_1\"])\n",
    "    test_preds = prod_model.predict(test_df[feature_cols])\n",
    "    test_mae   = mean_absolute_error(test_df[\"target_t_plus_1\"], test_preds)\n",
    "    test_mape  = mean_absolute_percentage_error(test_df[\"target_t_plus_1\"], test_preds)\n",
    "    print(f\"Hold‐out Test ({TEST_SIZE} mo): MAE={test_mae:.0f}, MAPE={test_mape:.2f}%\")\n",
    "\n",
    "    # 3) Retrain on full data and save\n",
    "    final_model = XGBRegressor(**tuned)\n",
    "    final_model.fit(monthly[feature_cols], monthly[\"target_t_plus_1\"])\n",
    "    joblib.dump(final_model, MODEL_PATH)\n",
    "    print(f\"Saved tuned model to {MODEL_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01: 182,819,312\n",
      "2025-02: 282,813,248\n",
      "2025-03: 462,352,448\n",
      "2025-04: 704,966,656\n",
      "2025-05: 721,588,736\n",
      "2025-06: 722,822,976\n",
      "2025-07: 1,795,055,232\n"
     ]
    }
   ],
   "source": [
    "# predict_pipeline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def load_and_preprocess(data_path):\n",
    "    \"\"\"\n",
    "    Load the CSV, filter dates, aggregate to monthly and\n",
    "    apply the same feature‐engineering/dropping as in training.\n",
    "    Returns the prepared `monthly` DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path, low_memory=False)\n",
    "    df[\"Kalendertag\"]    = pd.to_datetime(df[\"Kalendertag\"], errors=\"coerce\")\n",
    "    df[\"Eintrittsdatum\"] = pd.to_datetime(df[\"Eintrittsdatum\"], errors=\"coerce\")\n",
    "    df = df[df[\"Kalendertag\"] >= df[\"Eintrittsdatum\"]]\n",
    "    df[\"JahrMonat\"] = pd.to_datetime(df[\"Kalendertag\"].dt.to_period(\"M\").astype(str))\n",
    "\n",
    "    monthly = df.groupby(\"JahrMonat\").agg({\n",
    "        \"BIWNAV AV Neug Wesu\": \"sum\",\n",
    "        \"Anz. NeuFamilien\":     \"sum\",\n",
    "        \"Anz. Neukunden\":       \"sum\",\n",
    "        \"P AV neu\":             \"sum\",\n",
    "        \"SpB AV\":               \"sum\",\n",
    "    }).reset_index().rename(columns={\"BIWNAV AV Neug Wesu\": \"target\"})\n",
    "\n",
    "    monthly = monthly.sort_values(\"JahrMonat\").copy()\n",
    "    monthly[\"target_t\"]      = monthly[\"target\"]\n",
    "    monthly[\"month\"]         = monthly[\"JahrMonat\"].dt.month\n",
    "    monthly[\"month_sin\"]     = np.sin(2 * np.pi * monthly[\"month\"] / 12)\n",
    "    monthly[\"month_cos\"]     = np.cos(2 * np.pi * monthly[\"month\"] / 12)\n",
    "\n",
    "    for col in [\"Anz. NeuFamilien\",\"Anz. Neukunden\",\"P AV neu\",\"SpB AV\"]:\n",
    "        monthly[f\"{col}_lag1\"] = monthly[col].shift(1)\n",
    "        monthly[f\"{col}_lag2\"] = monthly[col].shift(2)\n",
    "\n",
    "    monthly[\"target_t_plus_1\"] = monthly[\"target\"].shift(-1)\n",
    "\n",
    "    # drop exactly the same columns as in training\n",
    "    monthly = monthly.drop(columns=[\n",
    "        \"month\",\n",
    "        \"Anz. NeuFamilien\",\"Anz. Neukunden\",\"P AV neu\",\"SpB AV\",\n",
    "        \"target\"\n",
    "    ])\n",
    "    return monthly.reset_index(drop=True)\n",
    "\n",
    "def forecast_periods(model_path, data_path, n_periods=7):\n",
    "    \"\"\"\n",
    "    Load the XGBoost model & data, then iteratively forecast `n_periods` ahead.\n",
    "    Returns a list of (JahrMonat (Timestamp), forecast_value).\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}. Please run training first.\")\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    monthly = load_and_preprocess(data_path)\n",
    "    feature_cols = [c for c in monthly.columns if c not in [\"JahrMonat\",\"target_t\",\"target_t_plus_1\"]]\n",
    "\n",
    "    # seed with last known features\n",
    "    last_feats = monthly.iloc[-1][feature_cols].copy()\n",
    "    last_date  = monthly.iloc[-1][\"JahrMonat\"]\n",
    "\n",
    "    forecasts = []\n",
    "    for _ in range(n_periods):\n",
    "        next_date = last_date + pd.DateOffset(months=1)\n",
    "\n",
    "        # update cycle\n",
    "        sin = np.sin(2 * np.pi * next_date.month / 12)\n",
    "        cos = np.cos(2 * np.pi * next_date.month / 12)\n",
    "        feats = last_feats.copy()\n",
    "        feats[\"month_sin\"], feats[\"month_cos\"] = sin, cos\n",
    "\n",
    "        # build 1×n DataFrame so sklearn sees names\n",
    "        X_pred_df = pd.DataFrame([feats], columns=feature_cols)\n",
    "        y_pred    = model.predict(X_pred_df)[0]\n",
    "\n",
    "        forecasts.append((next_date, y_pred))\n",
    "\n",
    "        # advance\n",
    "        last_date  = next_date\n",
    "        # if you need to feed predictions back into lags/target, do it here:\n",
    "        # e.g. last_feats[\"target_lag1\"] = last_feats[\"target_lag2\"]\n",
    "        #       last_feats[\"target_lag2\"] = y_pred\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # point to your tuned XGBoost artifact\n",
    "    MODEL_PATH = \"../models/global_xgb_model.pkl\"\n",
    "    DATA_PATH  = \"../data/4_PrognoseCase_AV_NG_-_Daten.csv\"\n",
    "    results    = forecast_periods(MODEL_PATH, DATA_PATH, n_periods=7)\n",
    "\n",
    "    for period, value in results:\n",
    "        print(f\"{period.strftime('%Y-%m')}: {value:,.0f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
